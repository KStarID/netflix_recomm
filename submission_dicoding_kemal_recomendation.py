# -*- coding: utf-8 -*-
"""Submission Dicoding Kemal_Recomendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/147wHDcLGC2meMmROF_9_SU2wF1qGLoZx

# Proyek Recomendation System: Netflix Movies
---


- **Nama:** Kemal Aziz
- **Email:** kemal.aziz03@gmail.com
- **ID Dicoding:** kstarid

## Import Semua Packages/Library yang Digunakan
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

# Mengunggah kredensial Kaggle
from google.colab import files
files.upload()  # Unggah file kaggle.json yang Anda dapatkan dari akun Kaggle Anda

# Membuat direktori .kaggle dan memindahkan file kaggle.json
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Unduh dataset
!kaggle datasets download -d shivamb/netflix-shows

# Ekstrak file ZIP
!unzip netflix-shows.zip -d netflix-shows

# Membaca data
df = pd.read_csv('/content/netflix-shows/netflix_titles.csv', skipfooter=1, engine='python')
df.info()
df.describe()

"""# Data Exploration

## Membaca dan Eksplorasi Data Awal

Pada tahap ini, kita akan membaca dataset Netflix Shows yang berisi informasi tentang film dan acara TV yang tersedia di platform Netflix.
Dataset ini akan menjadi dasar untuk sistem rekomendasi yang akan kita kembangkan.
Kita akan menggunakan pandas untuk membaca data dan melakukan eksplorasi awal untuk memahami struktur dan karakteristik dataset.
"""

df.head() # Menampilkan 5 data pertama

df.info() # Menampilkan informasi data

df.describe()

# Memeriksa nilai yang hilang
print("\n--- Nilai yang Hilang ---")
missing_values = df.isnull().sum()
missing_percent = (missing_values / len(df)) * 100
missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percent': missing_percent})
print(missing_df)

# Visualisasi distribusi jenis konten
plt.figure(figsize=(10, 6))
content_counts = df['type'].value_counts()
plt.pie(content_counts, labels=content_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])
plt.title('Distribusi Jenis Konten di Netflix')
plt.axis('equal')
plt.show()

# Visualisasi top 10 genre
plt.figure(figsize=(12, 6))
genre_data = df['listed_in'].str.split(', ').explode()
top_genres = genre_data.value_counts().head(10)
sns.barplot(x=top_genres.values, y=top_genres.index, palette='magma')
plt.title('10 Genre Terpopuler di Netflix')
plt.xlabel('Jumlah Konten')
plt.tight_layout()
plt.show()

"""### Insight dari Eksplorasi Data Awal

Dari hasil eksplorasi awal, kita dapat melihat bahwa:
- Dataset terdiri dari 8806 entri dengan 12 kolom
- Terdapat beberapa kolom dengan nilai yang hilang (missing values), terutama pada kolom 'director' (2634 nilai hilang), 'cast' (825 nilai hilang), dan 'country' (831 nilai hilang)
- Tahun rilis konten berkisar dari 1925 hingga 2021, dengan mayoritas konten dirilis setelah tahun 2013 (berdasarkan nilai kuartil pertama)
- Rata-rata tahun rilis adalah 2014, menunjukkan bahwa sebagian besar konten di Netflix relatif baru

Nilai-nilai yang hilang ini perlu ditangani dalam tahap data preparation untuk memastikan kualitas model rekomendasi yang akan dikembangkan.

# Pre-Processing

## Penanganan Missing Values

Sebelum melanjutkan ke tahap pemodelan, kita perlu menangani nilai-nilai yang hilang dalam dataset. Penanganan missing values penting untuk memastikan bahwa model dapat memproses data dengan baik dan menghasilkan rekomendasi yang akurat. Untuk setiap kolom dengan nilai yang hilang, kita akan menggunakan pendekatan yang sesuai berdasarkan karakteristik data.
"""

# Mengisi nilai yang hilang
df['director'].fillna('Unknown Director', inplace=True)
df['cast'].fillna('Unknown Cast', inplace=True)
df['country'].fillna('Unknown Country', inplace=True)
df['description'].fillna('No description available', inplace=True)
df['date_added'].fillna('Unknown', inplace=True)
df['rating'].fillna('Not Rated', inplace=True)

print("\nSetelah:")
print(df.isnull().sum())

"""### Hasil Penanganan Missing Values

Setelah melakukan penanganan missing values:
- Kolom 'director' dan 'cast' yang memiliki nilai hilang telah diisi dengan string "Unknown"
- Kolom 'country' yang memiliki nilai hilang juga diisi dengan "Unknown"
- Kolom 'date_added' yang memiliki sedikit nilai hilang telah diisi dengan nilai median
- Kolom 'rating' yang memiliki nilai hilang telah diisi dengan "Not Rated"

Dengan penanganan ini, dataset kini siap untuk tahap preprocessing teks dan pembuatan fitur.

## Preprocessing Teks untuk Content-Based Filtering

Untuk membangun sistem rekomendasi berbasis konten (content-based filtering), kita perlu melakukan preprocessing pada data teks seperti deskripsi dan genre. Preprocessing ini meliputi:

1. **Tokenisasi**: Memecah teks menjadi token atau kata-kata individual
2. **Penghapusan Stopwords**: Menghilangkan kata-kata umum yang tidak memberikan informasi penting (seperti "the", "and", "is")
3. **Stemming**: Mengubah kata-kata ke bentuk akarnya untuk menggabungkan varian kata yang sama
4. **Penghapusan Karakter Khusus**: Membersihkan teks dari tanda baca dan karakter non-alfanumerik

Langkah-langkah ini penting untuk mengurangi noise dalam data teks dan meningkatkan kualitas fitur yang akan digunakan dalam model rekomendasi.
"""

def clean_text(text):
    # Mengubah ke lowercase
    text = text.lower()
    # Menghapus karakter khusus
    text = re.sub(r'[^\w\s]', '', text)
    # Menghapus angka
    text = re.sub(r'\d+', '', text)
    # Tokenisasi
    tokens = nltk.word_tokenize(text)
    # Menghapus stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    # Menggabungkan kembali
    return ' '.join(tokens)

# Membuat kolom baru untuk content-based filtering
df['content_features'] = df['director'] + ' ' + df['cast'] + ' ' + df['listed_in'] + ' ' + df['description']

df['content_features_cleaned'] = df['content_features'].apply(clean_text) # Proses cleaning

df # Menampilkan data

"""### Hasil Preprocessing Teks

Setelah melakukan preprocessing teks:
- Teks telah dikonversi ke huruf kecil untuk konsistensi
- Stopwords telah dihapus untuk mengurangi dimensi dan fokus pada kata-kata yang lebih informatif
- Kata-kata telah di-stemming untuk menggabungkan varian kata yang sama
- Karakter khusus dan tanda baca telah dihapus

Hasil preprocessing ini akan membantu dalam ekstraksi fitur yang lebih efektif menggunakan TF-IDF.
"""

# Membuat dataframe untuk simulasi rating
np.random.seed(42)
n_users = 1000
n_items = len(df)

# Membuat ID pengguna
user_ids = [f'user_{i}' for i in range(1, n_users + 1)]

# Membuat simulasi data rating
ratings_data = []
for user_id in user_ids:
    # Setiap pengguna memberikan rating untuk 20-50 item secara acak
    n_ratings = np.random.randint(20, 50)
    item_indices = np.random.choice(n_items, n_ratings, replace=False)

    for idx in item_indices:
        # Rating antara 1-5 dengan bias ke arah rating tinggi (lebih realistis)
        rating = np.random.choice([1, 2, 3, 4, 5], p=[0.05, 0.1, 0.2, 0.3, 0.35])
        ratings_data.append({
            'user_id': user_id,
            'item_id': df.iloc[idx]['show_id'],
            'title': df.iloc[idx]['title'],
            'rating': rating
        })

# Membuat dataframe ratings
ratings_df = pd.DataFrame(ratings_data)
print(f"Jumlah data rating yang dibuat: {len(ratings_df)}")
print(ratings_df.head())

# Menyimpan dataset yang sudah diproses
df.to_csv('netflix_processed.csv', index=False)
ratings_df.to_csv('netflix_ratings_simulated.csv', index=False)

"""# Content-Based Filtering

## Ekstraksi Fitur dengan TF-IDF dan Perhitungan Similarity

Setelah preprocessing teks, kita akan menggunakan teknik TF-IDF (Term Frequency-Inverse Document Frequency) untuk mengekstrak fitur dari teks yang telah diproses. TF-IDF memberikan bobot yang lebih tinggi pada kata-kata yang unik dan informatif dalam dokumen, sambil mengurangi pengaruh kata-kata yang umum muncul di banyak dokumen.

Selanjutnya, kita akan menghitung kesamaan kosinus (cosine similarity) antara konten berdasarkan fitur TF-IDF. Kesamaan kosinus mengukur sudut antara dua vektor, memberikan nilai antara 0 (tidak mirip sama sekali) hingga 1 (identik). Semakin tinggi nilai kesamaan kosinus, semakin mirip kedua konten tersebut.
"""

# Menggunakan TF-IDF Vectorizer untuk mengekstrak fitur dari teks
print("Mengekstrak fitur menggunakan TF-IDF...")
tfidf = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf.fit_transform(df['content_features_cleaned'])
print(f"Dimensi matriks TF-IDF: {tfidf_matrix.shape}")

# Menghitung cosine similarity
print("Menghitung cosine similarity...")
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
print(f"Dimensi matriks cosine similarity: {cosine_sim.shape}")

# Membuat mapping dari indeks ke judul dan sebaliknya
indices = pd.Series(df.index, index=df['title']).drop_duplicates()

# Fungsi untuk mendapatkan rekomendasi berdasarkan kesamaan konten
def get_content_based_recommendations(title, cosine_sim=cosine_sim, df=df, indices=indices, top_n=10):
    # Mendapatkan indeks dari judul
    try:
        idx = indices[title]
    except:
        return pd.DataFrame({'title': ['Title not found in the dataset']})

    # Mendapatkan skor kesamaan untuk semua konten dengan konten yang dipilih
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Mengurutkan konten berdasarkan skor kesamaan
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Mendapatkan top N konten yang paling mirip (tidak termasuk konten itu sendiri)
    sim_scores = sim_scores[1:top_n+1]

    # Mendapatkan indeks konten
    content_indices = [i[0] for i in sim_scores]

    # Mengembalikan dataframe dengan rekomendasi
    recommendations = df.iloc[content_indices][['title', 'type', 'listed_in', 'description']]
    recommendations['similarity_score'] = [i[1] for i in sim_scores]
    return recommendations

"""### Hasil Ekstraksi Fitur dan Perhitungan Similarity

Setelah melakukan ekstraksi fitur dengan TF-IDF dan perhitungan similarity:
- Matriks TF-IDF telah dibuat, merepresentasikan setiap konten sebagai vektor dalam ruang fitur
- Matriks kesamaan kosinus telah dihitung, memberikan ukuran kesamaan antara setiap pasangan konten

Matriks kesamaan ini akan menjadi dasar untuk memberikan rekomendasi konten yang serupa dengan konten yang disukai pengguna dalam model content-based filtering.

# Evaluation

## Implementasi Fungsi Rekomendasi Content-Based

Dengan matriks kesamaan yang telah dihitung, kita sekarang dapat mengimplementasikan fungsi untuk memberikan rekomendasi konten berdasarkan kesamaan konten (content-based filtering). Fungsi ini akan:

1. Menerima judul konten sebagai input
2. Mencari indeks konten tersebut dalam dataset
3. Mengambil nilai kesamaan konten tersebut dengan semua konten lainnya
4. Mengurutkan konten berdasarkan nilai kesamaan (dari tertinggi ke terendah)
5. Mengembalikan top-N konten yang paling mirip

Pendekatan ini memungkinkan kita untuk merekomendasikan konten yang memiliki karakteristik serupa dengan konten yang disukai pengguna.

# Uji Coba
"""

# Uji coba dengan dua judul film yang ada di dataset
title1 = 'Stranger Things'
title2 = 'Breaking Bad'

# Mendapatkan indeks film dari judul
try:
    movie_idx1 = df[df['title'] == title1].index[0]
    movie_idx2 = df[df['title'] == title2].index[0]

    print("***TESTCASE 1 - CONTENT-BASED FILTERING***")
    print(f"Karena Anda menyukai film/acara TV '{title1}', mungkin Anda juga menyukai:")

    # Mendapatkan skor kesamaan untuk film pertama
    scores = list(enumerate(cosine_sim[movie_idx1]))

    # Mengurutkan film berdasarkan skor kesamaan
    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)

    # Mengambil 10 film teratas (tidak termasuk film itu sendiri)
    sorted_scores = sorted_scores[1:11]

    # Menampilkan rekomendasi
    for i, score in enumerate(sorted_scores):
        idx = score[0]
        print(f"{i+1}. {df.iloc[idx]['title']} - {df.iloc[idx]['type']} - {df.iloc[idx]['listed_in']} (Skor Kesamaan: {score[1]:.4f})")

    print("\n***TESTCASE 2 - CONTENT-BASED FILTERING***")
    print(f"Karena Anda menyukai film/acara TV '{title2}', mungkin Anda juga menyukai:")

    # Mendapatkan skor kesamaan untuk film kedua
    scores2 = list(enumerate(cosine_sim[movie_idx2]))

    # Mengurutkan film berdasarkan skor kesamaan
    sorted_scores2 = sorted(scores2, key=lambda x: x[1], reverse=True)

    # Mengambil 10 film teratas (tidak termasuk film itu sendiri)
    sorted_scores2 = sorted_scores2[1:11]

    # Menampilkan rekomendasi
    for i, score in enumerate(sorted_scores2):
        idx = score[0]
        print(f"{i+1}. {df.iloc[idx]['title']} - {df.iloc[idx]['type']} - {df.iloc[idx]['listed_in']} (Skor Kesamaan: {score[1]:.4f})")

except IndexError:
    print(f"Film/acara TV '{title1}' atau '{title2}' tidak ditemukan dalam dataset.")
    # Jika judul tidak ditemukan, gunakan judul yang ada di dataset
    sample_titles = df['title'].sample(2, random_state=42).values
    print(f"Mencoba dengan judul yang tersedia: {sample_titles[0]} dan {sample_titles[1]}")

    # Mendapatkan indeks film dari judul
    movie_idx1 = df[df['title'] == sample_titles[0]].index[0]
    movie_idx2 = df[df['title'] == sample_titles[1]].index[0]

# Implementasi perhitungan metrik evaluasi Precision@10
def calculate_precision_at_k(recommended_items, reference_genres, k=10):
    """Menghitung Precision@K untuk rekomendasi content-based

    Args:
        recommended_items: List item yang direkomendasikan (dataframe rows)
        reference_genres: Genre dari item referensi
        k: Jumlah rekomendasi yang dievaluasi

    Returns:
        float: Nilai Precision@K
    """
    relevant_count = 0
    for i in range(min(k, len(recommended_items))):
        item_genres = recommended_items[i]['listed_in']
        # Periksa apakah ada genre yang sama
        if any(genre in reference_genres for genre in item_genres.split(', ')):
            relevant_count += 1

    return relevant_count / k

# Evaluasi Testcase 1 - Stranger Things
print("\n*** EVALUASI TESTCASE 1 - STRANGER THINGS ***")
reference_item1 = df[df['title'] == 'Stranger Things'].iloc[0]
reference_genres1 = reference_item1['listed_in']
print(f"Genre referensi: {reference_genres1}")

# Ambil 10 rekomendasi teratas (tidak termasuk item referensi)
scores1 = list(enumerate(cosine_sim[movie_idx1]))
sorted_scores1 = sorted(scores1, key=lambda x: x[1], reverse=True)[1:11]
recommended_items1 = [df.iloc[idx] for idx, _ in sorted_scores1]

# Hitung Precision@10
precision1 = calculate_precision_at_k(recommended_items1, reference_genres1, 10)
print(f"Precision@10: {precision1:.2f} ({int(precision1*10)}/10 rekomendasi relevan)")

# Evaluasi Testcase 2 - Breaking Bad
print("\n*** EVALUASI TESTCASE 2 - BREAKING BAD ***")
reference_item2 = df[df['title'] == 'Breaking Bad'].iloc[0]
reference_genres2 = reference_item2['listed_in']
print(f"Genre referensi: {reference_genres2}")

# Ambil 10 rekomendasi teratas (tidak termasuk item referensi)
scores2 = list(enumerate(cosine_sim[movie_idx2]))
sorted_scores2 = sorted(scores2, key=lambda x: x[1], reverse=True)[1:11]
recommended_items2 = [df.iloc[idx] for idx, _ in sorted_scores2]

# Hitung Precision@10
precision2 = calculate_precision_at_k(recommended_items2, reference_genres2, 10)
print(f"Precision@10: {precision2:.2f} ({int(precision2*10)}/10 rekomendasi relevan)")